import os
import requests
from tqdm import tqdm
import numpy as np
from PIL import Image
import onnxruntime as ort
import sys
import time
from typing import List
from config import SCALE_FACTOR, QUANT_TYPE, ADVANCED_SPLIT


def download_file(url: str, save_path: str):
    """Download a file from a URL to a specified local path."""

    response = requests.get(url, stream=True)
    response.raise_for_status()  # Ensure we notice bad responses

    total = int(response.headers.get("content-length", 0))
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    with (
        open(save_path, "wb") as file,
        tqdm(
            desc=f"Downloading {os.path.basename(save_path)}",
            total=total,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as bar,
    ):
        for chunk in response.iter_content(chunk_size=8192):
            size = file.write(chunk)
            bar.update(size)

    print(f"Downloaded `{url}` to `{save_path}`.")


def download_file_if_not_exists(url: str, save_path: str):
    """Download a file only if it does not already exist."""
    if not os.path.exists(save_path):
        print(f"File `{save_path}` does not exist. Downloading...")
        download_file(url, save_path)
    else:
        # print(f"File {save_path} already exists. Skipping download.")
        pass


def download_models():
    """Download required model files if they do not exist."""
    # download_file_if_not_exists(
    #     "https://hf-mirror.com/Xenova/dinov2-small/resolve/main/onnx/model.onnx",
    #     "models/dinov2-small/model.onnx",
    # )
    download_file_if_not_exists(
        "https://hf-mirror.com/onnx-community/dinov2-with-registers-small/resolve/main/onnx/model.onnx",
        "models/dinov2-small/model.onnx",
    )


# def preprocess_image(image: Image.Image, target_size: int = 224) -> np.ndarray:
#     """é¢„å¤„ç†å›¾ç‰‡ä¸ºCLIPæ¨¡å‹è¾“å…¥ã€‚"""
#     image = image.resize((target_size, target_size))
#     arr = np.array(image).astype("float32") / 255.0
#     arr = (arr - 0.48145466) / 0.26862954  # Normalize
#     arr = np.transpose(arr, (2, 0, 1))  # HWC to CHW
#     arr = np.expand_dims(arr, axis=0)  # Add batch dimension
#     return arr


def preprocess_image(image: Image.Image, target_size: int = 224) -> np.ndarray:
    """é¢„å¤„ç†å›¾ç‰‡ä¸ºDINOv2æ¨¡å‹è¾“å…¥ã€‚"""
    # Resize + BICUBIC
    image = image.resize((target_size, target_size), Image.Resampling.BICUBIC)
    # è½¬ä¸ºRGB
    if image.mode != "RGB":
        image = image.convert("RGB")
    arr = np.array(image).astype("float32") / 255.0
    # ImageNetå½’ä¸€åŒ–å‚æ•° for DINOv2
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    arr = (arr - mean) / std
    arr = np.transpose(arr, (2, 0, 1))  # HWC to CHW
    arr = np.expand_dims(arr, axis=0)  # Add batch dimension
    arr = arr.astype("float32")  # ä¿è¯ä¸ºfloat32
    return arr


def generate_image_variants(image: Image.Image) -> List[Image.Image]:
    """ä¸ºä¸€å¼ å›¾ç‰‡ç”Ÿæˆ5ä¸ªå˜ä½“ï¼šåŸå§‹ + 4ä¸ªè§’è½è£å‰ªã€‚"""
    width, height = image.size
    # è£å‰ªå¤§å°ä¸ºå›¾ç‰‡çš„SCALE_FACTORå€
    crop_width = int(width * SCALE_FACTOR)
    crop_height = int(height * SCALE_FACTOR)
    variants = [image]  # åŸå§‹å›¾ç‰‡
    
    # å·¦ä¸Šè§’
    variants.append(image.crop((0, 0, crop_width, crop_height)))
    # å³ä¸Šè§’
    variants.append(image.crop((width - crop_width, 0, width, crop_height)))
    # å·¦ä¸‹è§’
    variants.append(image.crop((0, height - crop_height, crop_width, height)))
    # å³ä¸‹è§’
    variants.append(image.crop((width - crop_width, height - crop_height, width, height)))
    
    return variants


def extract_feature(image: Image.Image, model_path: str) -> np.ndarray:
    start_time = time.time()
    print("å¼€å§‹å›¾ç‰‡ç‰¹å¾æå– (DINOv2 ONNX)")

    # å›¾åƒé¢„å¤„ç†
    preprocess_start = time.time()
    input_tensor = preprocess_image(image)
    preprocess_time = time.time() - preprocess_start
    print(f"  â”œâ”€ å›¾åƒé¢„å¤„ç†è€—æ—¶: {preprocess_time:.3f}ç§’")

    # æ¨¡å‹æ¨ç†
    inference_start = time.time()
    session = ort.InferenceSession(model_path)
    outputs = session.run(None, {"pixel_values": input_tensor})
    # DINOv2è¾“å‡ºlast_hidden_stateï¼Œå½¢çŠ¶(1, 257, 384)ï¼Œæˆ‘ä»¬å–å¹³å‡
    # features = outputs[0].mean(axis=1).squeeze()  # (384,)
    features = outputs[0][:, 0, :].squeeze()  # å½¢çŠ¶: (384,)
    inference_time = time.time() - inference_start
    print(f"  â”œâ”€ å›¾åƒæ¨¡å‹æ¨ç†è€—æ—¶: {inference_time:.3f}ç§’")

    total_time = time.time() - start_time
    print(f"  â””â”€ å›¾ç‰‡ç‰¹å¾æå–æ€»è€—æ—¶: {total_time:.3f}ç§’")

    return features


def get_image(path: str) -> Image.Image:
    if path:
        return Image.open(path)
    else:
        # ä»å‰ªè´´æ¿è¯»å–å›¾ç‰‡
        try:
            from PIL import ImageGrab

            img = ImageGrab.grabclipboard()
            if isinstance(img, Image.Image):
                return img
            else:
                print("å‰ªè´´æ¿æ— å›¾ç‰‡ï¼")
                sys.exit(1)
        except Exception as e:
            print(f"å‰ªè´´æ¿è¯»å–å¤±è´¥: {e}")
            sys.exit(1)


def make_tokenizer(model_dir: str):
    """åˆ›å»ºå¹¶é…ç½®CLIP tokenizerã€‚"""
    tokenizer_file_path = os.path.join(model_dir, "tokenizer.json")

    try:
        tokenizer = Tokenizer.from_file(tokenizer_file_path)
        print("âœ… Tokenizer loaded successfully using the `tokenizers` library!")
    except Exception as e:
        print(
            f"âŒ Failed to load '{tokenizer_file_path}'. Please ensure the file exists and is valid."
        )
        print(f"Error details: {e}")
        sys.exit(1)

    # é…ç½®Padding
    pad_token = "[PAD]"
    pad_token_id = tokenizer.token_to_id(pad_token)
    if pad_token_id is not None:
        tokenizer.enable_padding(pad_id=pad_token_id, pad_token=pad_token)
        print(f"Padding enabled. PAD token ID: {pad_token_id}")
    else:
        print("Warning: [PAD] token not found.")

    # é…ç½®æˆªæ–­
    tokenizer.enable_truncation(max_length=77)

    return tokenizer


# def extract_text_feature(text: str, model_dir: str) -> np.ndarray:
#     """
#     ä½¿ç”¨å¤šè¯­è¨€CLIPæ¨¡å‹æå–æ–‡æœ¬ç‰¹å¾ã€‚
#     æ­¤ç‰ˆæœ¬ä¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨åç½®é¡¹ï¼ˆbiasï¼‰ï¼Œå¹¶è‡ªé€‚åº”å¤„ç†ã€‚
#     """
#     # è·¯å¾„å®šä¹‰
#     model_path = os.path.join(model_dir, "model_O4.onnx")
#     dense_path = os.path.join(model_dir, "2_Dense/model.safetensors")

#     # 1. Tokenization
#     tokenizer = make_tokenizer(model_dir)
#     encodings = tokenizer.encode_batch([text])

#     input_ids = np.array([enc.ids for enc in encodings], dtype=np.int64)
#     attention_mask = np.array([enc.attention_mask for enc in encodings], dtype=np.int64)

#     # 2. ONNX æ¨ç† (è·å– 768 ç»´è¯å‘é‡)
#     session = ort.InferenceSession(model_path)
#     outputs = session.run(None, {
#         "input_ids": input_ids,
#         "attention_mask": attention_mask
#     })
#     token_embeddings = outputs[0]  # shape: (1, 77, 768)

#     # 3. Mean Pooling (è®¡ç®—å¹³å‡æ± åŒ–)
#     mask_expanded = np.expand_dims(attention_mask, axis=-1).repeat(token_embeddings.shape[-1], axis=-1)
#     sum_embeddings = np.sum(token_embeddings * mask_expanded, axis=1)
#     sum_mask = np.clip(mask_expanded.sum(axis=1), a_min=1e-9, a_max=None)
#     pooled_feat_768 = sum_embeddings / sum_mask # shape: (1, 768)

#     # 4. Dense Layer (åº”ç”¨å…¨è¿æ¥å±‚)
#     dense_tensors = load_file(dense_path)
#     W = dense_tensors["linear.weight"]  # shape: (512, 768)

#     # æ ¸å¿ƒä¿®æ­£ï¼šåªè¿›è¡ŒçŸ©é˜µä¹˜æ³•
#     text_feat_512 = np.dot(pooled_feat_768, W.T)

#     # å¥å£®æ€§æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶ä¸­å­˜åœ¨ biasï¼Œåˆ™åŠ ä¸Šå®ƒ
#     if "linear.bias" in dense_tensors:
#         print("å‘ç°å¹¶åº”ç”¨åç½®é¡¹ (bias)ã€‚")
#         b = dense_tensors["linear.bias"]
#         text_feat_512 += b

#     return text_feat_512[0] # è¿”å› shape: (512,) çš„å‘é‡

# def extract_text_feature(text: str, model_dir: str) -> np.ndarray:
#     """
#     ä½¿ç”¨å¤šè¯­è¨€CLIPæ¨¡å‹æå–æ–‡æœ¬ç‰¹å¾ã€‚
#     æ­¤ç‰ˆæœ¬å°†æ± åŒ–ç­–ç•¥ä» Mean Pooling ä¿®æ”¹ä¸º CLS Poolingã€‚
#     """
#     # è·¯å¾„å®šä¹‰
#     model_path = os.path.join(model_dir, "model_O4.onnx")
#     dense_path = os.path.join(model_dir, "2_Dense/model.safetensors")

#     # 1. Tokenization
#     tokenizer = make_tokenizer(model_dir)
#     encodings = tokenizer.encode_batch([text])

#     input_ids = np.array([enc.ids for enc in encodings], dtype=np.int64)
#     attention_mask = np.array([enc.attention_mask for enc in encodings], dtype=np.int64)

#     # 2. ONNX æ¨ç† (è·å– 768 ç»´è¯å‘é‡)
#     session = ort.InferenceSession(model_path)
#     outputs = session.run(None, {
#         "input_ids": input_ids,
#         "attention_mask": attention_mask
#     })
#     token_embeddings = outputs[0]  # shape: (1, 77, 768)

#     # ------------------- æ ¸å¿ƒä¿®æ”¹ç‚¹ -------------------
#     # 3. CLS Pooling (è·å– [CLS] token çš„å‘é‡)
#     # [CLS] token æ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ª tokenã€‚
#     # token_embeddings[0] é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰å¥å­ã€‚
#     # [0] é€‰æ‹©è¯¥å¥å­çš„ç¬¬ä¸€ä¸ª tokenã€‚
#     pooled_feat_768 = token_embeddings[0][0] # shape: (768,)
#     # ---------------------------------------------------

#     # 4. Dense Layer (åº”ç”¨å…¨è¿æ¥å±‚)
#     dense_tensors = load_file(dense_path)
#     W = dense_tensors["linear.weight"]  # shape: (512, 768)

#     # å°† pooled_feat_768 å˜å› 2D array ä»¥ä¾¿è¿›è¡ŒçŸ©é˜µä¹˜æ³•
#     # np.dot((768,), (768, 512)) -> (512,)
#     text_feat_512 = np.dot(pooled_feat_768, W.T)

#     # æ£€æŸ¥å¹¶åº”ç”¨åç½®é¡¹ï¼ˆè™½ç„¶æ­¤æ¨¡å‹æ²¡æœ‰ï¼Œä½†è¿™æ˜¯å¥å£®çš„å†™æ³•ï¼‰
#     if "linear.bias" in dense_tensors:
#         b = dense_tensors["linear.bias"]
#         text_feat_512 += b

#     # æ³¨æ„ï¼štext_feat_512 æ­¤å¤„å·²ç»æ˜¯ (512,) çš„ä¸€ç»´å‘é‡ï¼Œæ— éœ€å†å–[0]
#     return text_feat_512


def quantize_features(features: np.ndarray, quant_type: str):
    """å¯¹ç‰¹å¾è¿›è¡Œé‡åŒ–ï¼Œè¿”å›é‡åŒ–åçš„ç‰¹å¾ï¼ˆå·²åé‡åŒ–å› float32ï¼‰ã€‚"""
    if quant_type == 'none':
        return features
    elif quant_type == 'float16':
        return features.astype(np.float16).astype(np.float32)
    else:
        return features
    """å¯¹ç‰¹å¾è¿›è¡Œé‡åŒ–ï¼Œè¿”å›é‡åŒ–åçš„ç‰¹å¾ï¼ˆå·²åé‡åŒ–å› float32ï¼‰ã€‚"""
    if quant_type == 'none':
        return features
    elif quant_type == 'float16':
        return features.astype(np.float16).astype(np.float32)
    else:
        return features


if __name__ == "__main__":
    # overall_start = time.time()

    # æ¨¡å‹ä¸‹è½½é˜¶æ®µ
    download_start = time.time()
    download_models()
    download_time = time.time() - download_start
    print(f"æ¨¡å‹ä¸‹è½½æ£€æŸ¥è€—æ—¶: {download_time:.3f}ç§’")
    print()

    user_input = input("è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼Œæˆ–ç•™ç©ºä»å‰ªåˆ‡æ¿è·å–å›¾ç‰‡ï¼š")

    # åŠ è½½è§†é¢‘å¸§ç‰¹å¾
    load_start = time.time()
    print("æ­£åœ¨åŠ è½½è§†é¢‘å¸§ç‰¹å¾...")
    video_feats = np.load("workdir/video_features.npy")  # shape: (num_frames, num_variants, 384) for DINOv2
    num_frames = video_feats.shape[0]
    num_variants = video_feats.shape[1]
    video_feats = video_feats / np.linalg.norm(video_feats, axis=2, keepdims=True)
    
    # é‡åŒ–
    video_feats = quantize_features(video_feats, QUANT_TYPE)
    
    load_time = time.time() - load_start
    print(f"è§†é¢‘ç‰¹å¾åŠ è½½è€—æ—¶: {load_time:.3f}ç§’")
    print(f"æ€»å¸§æ•°: {num_frames}ï¼Œæ¯å¸§{num_variants}ä¸ªå‘é‡ï¼Œé‡åŒ–ç±»å‹: {QUANT_TYPE}")

    # æŸ¥è¯¢ç‰¹å¾æå–é˜¶æ®µ
    if user_input.strip() == "":
        # å¤„ç†å›¾ç‰‡
        img = get_image("")
        query_feat = extract_feature(img, "models/dinov2-small/model.onnx")
    else:
        # å¤„ç†å›¾ç‰‡è·¯å¾„
        img = get_image(user_input)
        query_feat = extract_feature(img, "models/dinov2-small/model.onnx")

    # åå¤„ç†é˜¶æ®µ
    postprocess_start = time.time()
    print("å¼€å§‹åå¤„ç†...")

    # å½’ä¸€åŒ–
    normalization_start = time.time()
    query_feat = query_feat / np.linalg.norm(query_feat)
    normalization_time = time.time() - normalization_start
    print(f"  â”œâ”€ æŸ¥è¯¢ç‰¹å¾å½’ä¸€åŒ–è€—æ—¶: {normalization_time:.3f}ç§’")

    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰
    similarity_start = time.time()
    # video_feats: (num_frames, num_variants, 384), query_feat: (384,)
    sims = np.dot(video_feats, query_feat)  # (num_frames, num_variants)
    sims_flat = sims.flatten()  # (num_frames * num_variants,)
    similarity_time = time.time() - similarity_start
    print(f"  â”œâ”€ ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶: {similarity_time:.3f}ç§’")

    # æ’åºå’Œé€‰æ‹©Top-K
    topk_start = time.time()
    top_k = 20
    idxs_flat = np.argsort(sims_flat)[::-1][:top_k]
    # è½¬æ¢å›å¸§å·å’Œå‘é‡å·
    frame_indices = idxs_flat // num_variants
    variant_indices = idxs_flat % num_variants
    topk_time = time.time() - topk_start
    print(f"  â”œâ”€ Top-Kæ’åºè€—æ—¶: {topk_time:.3f}ç§’")

    postprocess_time = time.time() - postprocess_start
    print(f"  â””â”€ åå¤„ç†æ€»è€—æ—¶: {postprocess_time:.3f}ç§’")

    overall_time = time.time() - load_start
    print(f"\nğŸ¯ æ•´ä½“æ¨ç†æµç¨‹æ€»è€—æ—¶: {overall_time:.3f}ç§’")
    print("=" * 50)

    print("\nTop K ç›¸ä¼¼å¸§:")
    if ADVANCED_SPLIT:
        parts = ADVANCED_SPLIT.split(',')
        base_split = parts[0]
        combinations = parts[1:] if len(parts) > 1 else []
        h, w = map(int, base_split.split('x'))
        variant_names = [f"{i}.{j}" for i in range(h) for j in range(w)]
        for combo in combinations:
            combo = combo.strip('[]')
            variant_names.append(f"ç»„åˆ[{combo}]")
    else:
        variant_names = ["åŸå§‹", "å·¦ä¸Šè§’", "å³ä¸Šè§’", "å·¦ä¸‹è§’", "å³ä¸‹è§’"]
    for i, (frame_idx, variant_idx) in enumerate(zip(frame_indices, variant_indices)):
        sim = sims_flat[idxs_flat[i]]
        print(f"ç¬¬{frame_idx}å¸§ ({frame_idx // 60:02d}åˆ†{frame_idx % 60:02d}ç§’) - {variant_names[variant_idx]}ï¼Œç›¸ä¼¼åº¦: {sim:.4f}")
