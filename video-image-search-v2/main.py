import os
import requests
from tqdm import tqdm
import numpy as np
from PIL import Image
import onnxruntime as ort
import sys
import time
from tokenizers import Tokenizer
from safetensors.numpy import load_file  # éœ€è¦ pip install safetensors
import ml_dtypes
from typing import List
from config import SCALE_FACTOR, QUANT_TYPE, ADVANCED_SPLIT


def download_file(url: str, save_path: str):
    """Download a file from a URL to a specified local path."""

    response = requests.get(url, stream=True)
    response.raise_for_status()  # Ensure we notice bad responses

    total = int(response.headers.get("content-length", 0))
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    with (
        open(save_path, "wb") as file,
        tqdm(
            desc=f"Downloading {os.path.basename(save_path)}",
            total=total,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as bar,
    ):
        for chunk in response.iter_content(chunk_size=8192):
            size = file.write(chunk)
            bar.update(size)

    print(f"Downloaded `{url}` to `{save_path}`.")


def download_file_if_not_exists(url: str, save_path: str):
    """Download a file only if it does not already exist."""
    if not os.path.exists(save_path):
        print(f"File `{save_path}` does not exist. Downloading...")
        download_file(url, save_path)
    else:
        # print(f"File {save_path} already exists. Skipping download.")
        pass


def download_models():
    """Download required model files if they do not exist."""
    # download_file_if_not_exists(
    #     "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/onnx/model_quint8_avx2.onnx",
    #     "models/clip-ViT-B-32-multilingual-v1/model_quint8_avx2.onnx",
    # )
    download_file_if_not_exists(
        "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/onnx/model_O4.onnx",
        "models/clip-ViT-B-32-multilingual-v1/model_O4.onnx",
    )
    download_file_if_not_exists(
        "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/2_Dense/model.safetensors",
        "models/clip-ViT-B-32-multilingual-v1/2_Dense/model.safetensors",
    )
    download_file_if_not_exists(
        "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/tokenizer.json",
        "models/clip-ViT-B-32-multilingual-v1/tokenizer.json",
    )
    # download_file_if_not_exists(
    #     "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/tokenizer_config.json",
    #     "models/clip-ViT-B-32-multilingual-v1/tokenizer_config.json",
    # )
    # download_file_if_not_exists(
    #     "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/vocab.txt",
    #     "models/clip-ViT-B-32-multilingual-v1/vocab.txt",
    # )
    # download_file_if_not_exists(
    #     "https://hf-mirror.com/sentence-transformers/clip-ViT-B-32-multilingual-v1/resolve/main/special_tokens_map.json",
    #     "models/clip-ViT-B-32-multilingual-v1/special_tokens_map.json",
    # )
    download_file_if_not_exists(
        "https://hf-mirror.com/Qdrant/clip-ViT-B-32-vision/resolve/main/model.onnx",
        "models/clip-ViT-B-32-vision/model.onnx",
    )


# def preprocess_image(image: Image.Image, target_size: int = 224) -> np.ndarray:
#     """é¢„å¤„ç†å›¾ç‰‡ä¸ºCLIPæ¨¡å‹è¾“å…¥ã€‚"""
#     image = image.resize((target_size, target_size))
#     arr = np.array(image).astype("float32") / 255.0
#     arr = (arr - 0.48145466) / 0.26862954  # Normalize
#     arr = np.transpose(arr, (2, 0, 1))  # HWC to CHW
#     arr = np.expand_dims(arr, axis=0)  # Add batch dimension
#     return arr


def preprocess_image(image: Image.Image, target_size: int = 224) -> np.ndarray:
    """é¢„å¤„ç†å›¾ç‰‡ä¸ºCLIPæ¨¡å‹è¾“å…¥ï¼Œä»¿ç…§å®˜æ–¹_transformã€‚"""
    # Resize + BICUBIC
    image = image.resize((target_size, target_size), Image.Resampling.BICUBIC)
    # è½¬ä¸ºRGB
    if image.mode != "RGB":
        image = image.convert("RGB")
    arr = np.array(image).astype("float32") / 255.0
    # å®˜æ–¹å½’ä¸€åŒ–å‚æ•°
    mean = np.array([0.48145466, 0.4578275, 0.40821073])
    std = np.array([0.26862954, 0.26130258, 0.27577711])
    arr = (arr - mean) / std
    arr = np.transpose(arr, (2, 0, 1))  # HWC to CHW
    arr = np.expand_dims(arr, axis=0)  # Add batch dimension
    arr = arr.astype("float32")  # ä¿è¯ä¸ºfloat32
    return arr


def generate_image_variants(image: Image.Image) -> List[Image.Image]:
    """ä¸ºä¸€å¼ å›¾ç‰‡ç”Ÿæˆ5ä¸ªå˜ä½“ï¼šåŸå§‹ + 4ä¸ªè§’è½è£å‰ªã€‚"""
    width, height = image.size
    # è£å‰ªå¤§å°ä¸ºå›¾ç‰‡çš„SCALE_FACTORå€
    crop_width = int(width * SCALE_FACTOR)
    crop_height = int(height * SCALE_FACTOR)
    variants = [image]  # åŸå§‹å›¾ç‰‡
    
    # å·¦ä¸Šè§’
    variants.append(image.crop((0, 0, crop_width, crop_height)))
    # å³ä¸Šè§’
    variants.append(image.crop((width - crop_width, 0, width, crop_height)))
    # å·¦ä¸‹è§’
    variants.append(image.crop((0, height - crop_height, crop_width, height)))
    # å³ä¸‹è§’
    variants.append(image.crop((width - crop_width, height - crop_height, width, height)))
    
    return variants


def extract_feature(image: Image.Image, model_path: str) -> np.ndarray:
    start_time = time.time()
    print("å¼€å§‹å›¾ç‰‡ç‰¹å¾æå–")

    # å›¾åƒé¢„å¤„ç†
    preprocess_start = time.time()
    input_tensor = preprocess_image(image)
    preprocess_time = time.time() - preprocess_start
    print(f"  â”œâ”€ å›¾åƒé¢„å¤„ç†è€—æ—¶: {preprocess_time:.3f}ç§’")

    # æ¨¡å‹æ¨ç†
    inference_start = time.time()
    session = ort.InferenceSession(model_path)
    outputs = session.run(None, {"pixel_values": input_tensor})
    inference_time = time.time() - inference_start
    print(f"  â”œâ”€ å›¾åƒæ¨¡å‹æ¨ç†è€—æ—¶: {inference_time:.3f}ç§’")

    total_time = time.time() - start_time
    print(f"  â””â”€ å›¾ç‰‡ç‰¹å¾æå–æ€»è€—æ—¶: {total_time:.3f}ç§’")

    return outputs[0][0]


def get_image(path: str) -> Image.Image:
    if path:
        return Image.open(path)
    else:
        # ä»å‰ªè´´æ¿è¯»å–å›¾ç‰‡
        try:
            from PIL import ImageGrab

            img = ImageGrab.grabclipboard()
            if isinstance(img, Image.Image):
                return img
            else:
                print("å‰ªè´´æ¿æ— å›¾ç‰‡ï¼")
                sys.exit(1)
        except Exception as e:
            print(f"å‰ªè´´æ¿è¯»å–å¤±è´¥: {e}")
            sys.exit(1)


def make_tokenizer(model_dir: str):
    """åˆ›å»ºå¹¶é…ç½®CLIP tokenizerã€‚"""
    tokenizer_file_path = os.path.join(model_dir, "tokenizer.json")

    try:
        tokenizer = Tokenizer.from_file(tokenizer_file_path)
        print("âœ… Tokenizer loaded successfully using the `tokenizers` library!")
    except Exception as e:
        print(
            f"âŒ Failed to load '{tokenizer_file_path}'. Please ensure the file exists and is valid."
        )
        print(f"Error details: {e}")
        sys.exit(1)

    # é…ç½®Padding
    pad_token = "[PAD]"
    pad_token_id = tokenizer.token_to_id(pad_token)
    if pad_token_id is not None:
        tokenizer.enable_padding(pad_id=pad_token_id, pad_token=pad_token)
        print(f"Padding enabled. PAD token ID: {pad_token_id}")
    else:
        print("Warning: [PAD] token not found.")

    # é…ç½®æˆªæ–­
    tokenizer.enable_truncation(max_length=77)

    return tokenizer


# def extract_text_feature(text: str, model_dir: str) -> np.ndarray:
#     """
#     ä½¿ç”¨å¤šè¯­è¨€CLIPæ¨¡å‹æå–æ–‡æœ¬ç‰¹å¾ã€‚
#     æ­¤ç‰ˆæœ¬ä¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨åç½®é¡¹ï¼ˆbiasï¼‰ï¼Œå¹¶è‡ªé€‚åº”å¤„ç†ã€‚
#     """
#     # è·¯å¾„å®šä¹‰
#     model_path = os.path.join(model_dir, "model_O4.onnx")
#     dense_path = os.path.join(model_dir, "2_Dense/model.safetensors")

#     # 1. Tokenization
#     tokenizer = make_tokenizer(model_dir)
#     encodings = tokenizer.encode_batch([text])

#     input_ids = np.array([enc.ids for enc in encodings], dtype=np.int64)
#     attention_mask = np.array([enc.attention_mask for enc in encodings], dtype=np.int64)

#     # 2. ONNX æ¨ç† (è·å– 768 ç»´è¯å‘é‡)
#     session = ort.InferenceSession(model_path)
#     outputs = session.run(None, {
#         "input_ids": input_ids,
#         "attention_mask": attention_mask
#     })
#     token_embeddings = outputs[0]  # shape: (1, 77, 768)

#     # 3. Mean Pooling (è®¡ç®—å¹³å‡æ± åŒ–)
#     mask_expanded = np.expand_dims(attention_mask, axis=-1).repeat(token_embeddings.shape[-1], axis=-1)
#     sum_embeddings = np.sum(token_embeddings * mask_expanded, axis=1)
#     sum_mask = np.clip(mask_expanded.sum(axis=1), a_min=1e-9, a_max=None)
#     pooled_feat_768 = sum_embeddings / sum_mask # shape: (1, 768)

#     # 4. Dense Layer (åº”ç”¨å…¨è¿æ¥å±‚)
#     dense_tensors = load_file(dense_path)
#     W = dense_tensors["linear.weight"]  # shape: (512, 768)

#     # æ ¸å¿ƒä¿®æ­£ï¼šåªè¿›è¡ŒçŸ©é˜µä¹˜æ³•
#     text_feat_512 = np.dot(pooled_feat_768, W.T)

#     # å¥å£®æ€§æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶ä¸­å­˜åœ¨ biasï¼Œåˆ™åŠ ä¸Šå®ƒ
#     if "linear.bias" in dense_tensors:
#         print("å‘ç°å¹¶åº”ç”¨åç½®é¡¹ (bias)ã€‚")
#         b = dense_tensors["linear.bias"]
#         text_feat_512 += b

#     return text_feat_512[0] # è¿”å› shape: (512,) çš„å‘é‡

# def extract_text_feature(text: str, model_dir: str) -> np.ndarray:
#     """
#     ä½¿ç”¨å¤šè¯­è¨€CLIPæ¨¡å‹æå–æ–‡æœ¬ç‰¹å¾ã€‚
#     æ­¤ç‰ˆæœ¬å°†æ± åŒ–ç­–ç•¥ä» Mean Pooling ä¿®æ”¹ä¸º CLS Poolingã€‚
#     """
#     # è·¯å¾„å®šä¹‰
#     model_path = os.path.join(model_dir, "model_O4.onnx")
#     dense_path = os.path.join(model_dir, "2_Dense/model.safetensors")

#     # 1. Tokenization
#     tokenizer = make_tokenizer(model_dir)
#     encodings = tokenizer.encode_batch([text])

#     input_ids = np.array([enc.ids for enc in encodings], dtype=np.int64)
#     attention_mask = np.array([enc.attention_mask for enc in encodings], dtype=np.int64)

#     # 2. ONNX æ¨ç† (è·å– 768 ç»´è¯å‘é‡)
#     session = ort.InferenceSession(model_path)
#     outputs = session.run(None, {
#         "input_ids": input_ids,
#         "attention_mask": attention_mask
#     })
#     token_embeddings = outputs[0]  # shape: (1, 77, 768)

#     # ------------------- æ ¸å¿ƒä¿®æ”¹ç‚¹ -------------------
#     # 3. CLS Pooling (è·å– [CLS] token çš„å‘é‡)
#     # [CLS] token æ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ª tokenã€‚
#     # token_embeddings[0] é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰å¥å­ã€‚
#     # [0] é€‰æ‹©è¯¥å¥å­çš„ç¬¬ä¸€ä¸ª tokenã€‚
#     pooled_feat_768 = token_embeddings[0][0] # shape: (768,)
#     # ---------------------------------------------------

#     # 4. Dense Layer (åº”ç”¨å…¨è¿æ¥å±‚)
#     dense_tensors = load_file(dense_path)
#     W = dense_tensors["linear.weight"]  # shape: (512, 768)

#     # å°† pooled_feat_768 å˜å› 2D array ä»¥ä¾¿è¿›è¡ŒçŸ©é˜µä¹˜æ³•
#     # np.dot((768,), (768, 512)) -> (512,)
#     text_feat_512 = np.dot(pooled_feat_768, W.T)

#     # æ£€æŸ¥å¹¶åº”ç”¨åç½®é¡¹ï¼ˆè™½ç„¶æ­¤æ¨¡å‹æ²¡æœ‰ï¼Œä½†è¿™æ˜¯å¥å£®çš„å†™æ³•ï¼‰
#     if "linear.bias" in dense_tensors:
#         b = dense_tensors["linear.bias"]
#         text_feat_512 += b

#     # æ³¨æ„ï¼štext_feat_512 æ­¤å¤„å·²ç»æ˜¯ (512,) çš„ä¸€ç»´å‘é‡ï¼Œæ— éœ€å†å–[0]
#     return text_feat_512


def extract_text_feature(text: str, model_dir: str) -> np.ndarray:
    """
    æœ€ç»ˆä¿®æ­£ç‰ˆï¼š
    1. ä½¿ç”¨é…ç½®æ–‡ä»¶æ˜ç¡®è§„å®šçš„ Mean Pooling ç­–ç•¥ã€‚
    2. å¢åŠ äº†ä¸€ä¸ªåœ¨æ± åŒ–åã€Denseå±‚å‰çš„ L2 Normalization æ­¥éª¤ã€‚
    """
    start_time = time.time()
    print(f"å¼€å§‹æ–‡æœ¬ç‰¹å¾æå–: '{text}'")

    # ... (å‰é¢çš„è·¯å¾„å®šä¹‰ã€Tokenizationã€ONNXæ¨ç†éƒ¨åˆ†ä¿æŒä¸å˜) ...
    # ... (ç¡®ä¿ tokenizer å’Œ onnx session çš„åŠ è½½æ˜¯æ­£ç¡®çš„) ...
    model_path = os.path.join(model_dir, "model_O4.onnx")
    # model_path = os.path.join(model_dir, "model_quint8_avx2.onnx")
    dense_path = os.path.join(model_dir, "2_Dense/model.safetensors")

    # 1. Tokenization
    tokenization_start = time.time()
    tokenizer = make_tokenizer(model_dir)
    encodings = tokenizer.encode_batch([text])
    input_ids = np.array([enc.ids for enc in encodings], dtype=np.int64)
    attention_mask = np.array([enc.attention_mask for enc in encodings], dtype=np.int64)
    tokenization_time = time.time() - tokenization_start
    print(f"  â”œâ”€ æ–‡æœ¬åˆ†è¯è€—æ—¶: {tokenization_time:.3f}ç§’")

    # 2. ONNX æ¨ç†
    inference_start = time.time()
    session = ort.InferenceSession(model_path)
    outputs = session.run(
        None, {"input_ids": input_ids, "attention_mask": attention_mask}
    )
    token_embeddings = outputs[0]  # shape: (1, 77, 768)
    inference_time = time.time() - inference_start
    print(f"  â”œâ”€ ONNXæ¨¡å‹æ¨ç†è€—æ—¶: {inference_time:.3f}ç§’")

    # 3. Mean Pooling (æ ¹æ®æ¨¡å‹é…ç½®ï¼Œè¿™æ˜¯å”¯ä¸€æ­£ç¡®çš„ç­–ç•¥)
    pooling_start = time.time()
    mask_expanded = np.expand_dims(attention_mask, axis=-1).repeat(
        token_embeddings.shape[-1], axis=-1
    )
    sum_embeddings = np.sum(token_embeddings * mask_expanded, axis=1)
    sum_mask = np.clip(mask_expanded.sum(axis=1), a_min=1e-9, a_max=None)
    pooled_feat_768 = sum_embeddings / sum_mask  # shape: (1, 768)

    # ------------------- å¢åŠ çš„å…³é”®æ­¥éª¤ -------------------
    # 4. L2 Normalization (åœ¨é€å…¥Denseå±‚å‰è¿›è¡Œå½’ä¸€åŒ–)
    # è¿™æ˜¯CLIPæ¶æ„ä¸­çš„ä¸€ä¸ªå¸¸è§æ“ä½œï¼Œå¾ˆå¯èƒ½æ˜¯è¢«æˆ‘ä»¬å¿½ç•¥çš„å…³é”®
    norm = np.linalg.norm(pooled_feat_768, axis=1, keepdims=True)
    pooled_feat_768_normalized = pooled_feat_768 / norm
    # ---------------------------------------------------

    # 5. Dense Layer (ä½¿ç”¨å½’ä¸€åŒ–åçš„ç‰¹å¾)
    dense_tensors = load_file(dense_path)
    W = dense_tensors["linear.weight"]

    # ä½¿ç”¨å½’ä¸€åŒ–åçš„pooled_feat_768_normalizedè¿›è¡Œè®¡ç®—
    text_feat_512 = np.dot(pooled_feat_768_normalized, W.T)

    if "linear.bias" in dense_tensors:  # å¯¹äºæ­¤æ¨¡å‹ï¼Œè¿™éƒ¨åˆ†ä¸ä¼šæ‰§è¡Œ
        b = dense_tensors["linear.bias"]
        text_feat_512 += b

    pooling_time = time.time() - pooling_start
    print(f"  â”œâ”€ ç‰¹å¾æ± åŒ–å’ŒæŠ•å½±è€—æ—¶: {pooling_time:.3f}ç§’")

    total_time = time.time() - start_time
    print(f"  â””â”€ æ–‡æœ¬ç‰¹å¾æå–æ€»è€—æ—¶: {total_time:.3f}ç§’")

    return text_feat_512[0]


def quantize_features(features: np.ndarray, quant_type: str):
    """å¯¹ç‰¹å¾è¿›è¡Œé‡åŒ–ï¼Œè¿”å›é‡åŒ–åçš„ç‰¹å¾ï¼ˆå·²åé‡åŒ–å› float32ï¼‰ã€‚"""
    if quant_type == 'none':
        return features
    elif quant_type == 'float16':
        return features.astype(np.float16).astype(np.float32)
    elif quant_type == 'bfloat16':
        return features.astype(ml_dtypes.bfloat16).astype(np.float32)
    elif quant_type == 'float8_e4m3':
        return features.astype(ml_dtypes.float8_e4m3).astype(np.float32)
    elif quant_type == 'int8':
        # å¯¹ç§°é‡åŒ–ï¼Œå‡è®¾èŒƒå›´ [-1, 1]
        scale = 127.0  # max_abs / 127, max_abs=1
        quantized = np.round(features / (1.0 / scale)).astype(np.int8)
        # ç«‹å³åé‡åŒ–
        dequantized = quantized.astype(np.float32) * (1.0 / scale)
        return dequantized
    else:
        raise ValueError(f"Unsupported quant_type: {quant_type}")


if __name__ == "__main__":
    # overall_start = time.time()

    # æ¨¡å‹ä¸‹è½½é˜¶æ®µ
    download_start = time.time()
    download_models()
    download_time = time.time() - download_start
    print(f"æ¨¡å‹ä¸‹è½½æ£€æŸ¥è€—æ—¶: {download_time:.3f}ç§’")
    print()

    user_input = input("è¾“å…¥æ–‡æœ¬å†…å®¹ï¼Œæˆ–ç•™ç©ºä»å‰ªåˆ‡æ¿è·å–å›¾ç‰‡ï¼š")

    # åŠ è½½è§†é¢‘å¸§ç‰¹å¾
    load_start = time.time()
    print("æ­£åœ¨åŠ è½½è§†é¢‘å¸§ç‰¹å¾...")
    video_feats = np.load("workdir/video_features.npy")  # shape: (num_frames, num_variants, 512)
    num_frames = video_feats.shape[0]
    num_variants = video_feats.shape[1]
    video_feats = video_feats / np.linalg.norm(video_feats, axis=2, keepdims=True)
    
    # é‡åŒ–
    video_feats = quantize_features(video_feats, QUANT_TYPE)
    
    load_time = time.time() - load_start
    print(f"è§†é¢‘ç‰¹å¾åŠ è½½è€—æ—¶: {load_time:.3f}ç§’")
    print(f"æ€»å¸§æ•°: {num_frames}ï¼Œæ¯å¸§{num_variants}ä¸ªå‘é‡ï¼Œé‡åŒ–ç±»å‹: {QUANT_TYPE}")

    # æŸ¥è¯¢ç‰¹å¾æå–é˜¶æ®µ
    if user_input.strip() == "":
        # å¤„ç†å›¾ç‰‡
        img = get_image("")
        model_path = "models/clip-ViT-B-32-vision/model.onnx"
        query_feat = extract_feature(img, model_path)
    else:
        # å¤„ç†æ–‡æœ¬
        model_dir = "models/clip-ViT-B-32-multilingual-v1"
        query_feat = extract_text_feature(user_input, model_dir)

    # åå¤„ç†é˜¶æ®µ
    postprocess_start = time.time()
    print("å¼€å§‹åå¤„ç†...")

    # å½’ä¸€åŒ–
    normalization_start = time.time()
    query_feat = query_feat / np.linalg.norm(query_feat)
    normalization_time = time.time() - normalization_start
    print(f"  â”œâ”€ æŸ¥è¯¢ç‰¹å¾å½’ä¸€åŒ–è€—æ—¶: {normalization_time:.3f}ç§’")

    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰
    similarity_start = time.time()
    # video_feats: (num_frames, num_variants, 512), query_feat: (512,)
    sims = np.dot(video_feats, query_feat)  # (num_frames, num_variants)
    sims_flat = sims.flatten()  # (num_frames * num_variants,)
    similarity_time = time.time() - similarity_start
    print(f"  â”œâ”€ ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶: {similarity_time:.3f}ç§’")

    # æ’åºå’Œé€‰æ‹©Top-K
    topk_start = time.time()
    top_k = 20
    idxs_flat = np.argsort(sims_flat)[::-1][:top_k]
    # è½¬æ¢å›å¸§å·å’Œå‘é‡å·
    frame_indices = idxs_flat // num_variants
    variant_indices = idxs_flat % num_variants
    topk_time = time.time() - topk_start
    print(f"  â”œâ”€ Top-Kæ’åºè€—æ—¶: {topk_time:.3f}ç§’")

    postprocess_time = time.time() - postprocess_start
    print(f"  â””â”€ åå¤„ç†æ€»è€—æ—¶: {postprocess_time:.3f}ç§’")

    overall_time = time.time() - load_start
    print(f"\nğŸ¯ æ•´ä½“æ¨ç†æµç¨‹æ€»è€—æ—¶: {overall_time:.3f}ç§’")
    print("=" * 50)

    print("\nTop K ç›¸ä¼¼å¸§:")
    if ADVANCED_SPLIT:
        parts = ADVANCED_SPLIT.split(',')
        base_split = parts[0]
        combinations = parts[1:] if len(parts) > 1 else []
        h, w = map(int, base_split.split('x'))
        variant_names = [f"{i}.{j}" for i in range(h) for j in range(w)]
        for combo in combinations:
            combo = combo.strip('[]')
            variant_names.append(f"ç»„åˆ[{combo}]")
    else:
        variant_names = ["åŸå§‹", "å·¦ä¸Šè§’", "å³ä¸Šè§’", "å·¦ä¸‹è§’", "å³ä¸‹è§’"]
    for i, (frame_idx, variant_idx) in enumerate(zip(frame_indices, variant_indices)):
        sim = sims_flat[idxs_flat[i]]
        print(f"ç¬¬{frame_idx}å¸§ ({frame_idx // 60:02d}åˆ†{frame_idx % 60:02d}ç§’) - {variant_names[variant_idx]}ï¼Œç›¸ä¼¼åº¦: {sim:.4f}")
